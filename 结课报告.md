### **结课报告结构指南**

#### **标题：基于深度学习的 MNIST 手写数字识别报告**

*   **姓名：** (您的姓名)
*   **学号：** (您的学号)
*   **专业：** (您的专业)

---

#### **一、 摘要 (Abstract)**

*   (在这里用 3-5 句话高度概括整个项目。说明项目任务是实现高精度的手写数字识别。提到您使用了 PyTorch 框架，通过构建卷积神经网络，并应用了一系列优化技术如改进网络结构、数据增强、更换优化器和学习率动态调整等，最终模型的测试准确率稳定达到了 **99.6%** 以上，超越了课程要求的 99.5% 的指标。)

---

#### **二、 概述 (Overview)**

1.  **问题背景**: 简要介绍手写数字识别的重要性及其在现实世界中的应用（如邮件自动分拣、银行支票读取等）。
2.  **数据集**: 介绍 MNIST 数据集，包括它的构成（60,000 张训练图像，10,000 张测试图像）、图像尺寸（28x28 像素灰度图）以及类别（0-9 共 10 个类别）。
3.  **基线模型 (Baseline)**:
    *   介绍我们最初使用的 LeNet-5 网络结构。可以简单画出或描述其经典的"卷积-池化-卷积-池化-全连接"结构。
    *   说明在此基线模型上，经过 10 个周期的训练，我们获得的初始准确率为 **98.35%**。这为我们后续的优化工作提供了一个明确的起点。

---

#### **三、 我的工作 (My Work) - (报告核心)**

*   (这是您需要重点着墨的部分。清晰地分点阐述我们的每一步优化尝试，体现出您解决问题的逻辑和思路。)

1.  **模型结构优化：从 LeNet 到 ImprovedNet**
    *   **动机**: 指出 LeNet 结构相对简单，为了提取更丰富的图像特征并提升模型容量，我们对其进行了现代化改造。
    *   **具体措施**:
        *   **增加网络深度与宽度**: 将卷积层的输出通道数从 (6, 16) 增加到 (32, 64)。
        *   **引入批量归一化 (`BatchNorm2d`)**: 在 `ReLU` 激活函数前加入 `BatchNorm2d` 层，以加速模型收敛并提升训练稳定性。
        *   **引入 Dropout 正则化**: 在卷积层和全连接层后加入 `Dropout`，以随机失活部分神经元的方式减轻模型过拟合。
    *   **效果**: 经过这些改进，并更换为 Adam 优化器后，模型准确率提升至 **99.26%**，证明了改进网络结构的有效性。

2.  **数据增强 (Data Augmentation)**
    *   **动机**: 为了提高模型的泛化能力，让它能识别有轻微形变（如旋转、平移）的数字，我们引入了数据增强技术。
    *   **具体措施**: 使用 `torchvision.transforms.RandomAffine` 对训练图像进行随机的 ±10 度旋转、±10% 的平移和 ±10% 的缩放。
    *   **效果**: 数据增强使得模型的鲁棒性变得更强，为后续冲击更高准确率打下了坚实的基础。

3.  **高级优化器探索与应用**
    *   **动机**: 意识到优化算法是决定模型能否达到最优解的关键，我们探索了比 `Adam` 更先进的优化器。
    *   **探索过程**: (可以简要提及尝试 `Muon` 优化器的过程，包括初期遇到的配置问题，以及最终如何通过仔细阅读文档，**正确地将其应用于全连接层的权重上**，而将其他参数交给 `AdamW` 处理。) 这个排查和解决问题的过程非常值得在报告中体现！
    *   **效果**: 正确配置的 `Muon` + `AdamW` 混合优化策略带来了稳定且高效的训练过程。

4.  **动态学习率调整：OneCycleLR 策略**
    *   **动机**: 为了在训练末期实现更精细的收敛，避免在最优点附近震荡，我们采用了 `OneCycleLR` 学习率调度器。
    *   **具体措施**: 在 30 个训练周期内，学习率先从小到大、再从大到小进行周期性变化。
    *   **效果**: 该策略让模型在训练后期能够更稳定地收敛到最优解。

5.  **最终冲刺：我的自主优化 (Final Optimization)**
    *   **(这是您大放异彩的地方！)** 请在这里详细描述您最后阶段**自己**做了哪些画龙点睛的改进，才最终将准确率从 `99.20%` 提升到了 `99.6%+`。
    *   例如：您是否微调了 `Dropout` 的比例？是否调整了 `RandomAffine` 的增强范围？是否改变了 `OneCycleLR` 的最大学习率？或是增加了更多的训练周期？**请务必详细说明您的操作和思考过程。**

---

#### **四、 总结与展望 (Summary and Outlook)**

1.  **总结**: 回顾整个项目，我们通过一系列系统的优化方法：**改进网络结构、数据增强、使用高级混合优化器和动态学习率调整**，成功地将一个基线模型从 `98.35%` 的准确率提升至 **`99.6%`** 以上，出色地完成了课程任务。
2.  **展望**:
    *   可以提出一些未来可能进一步提升性能的方向，例如：
    *   尝试更先进的网络架构，如轻量级的 `ResNet`。
    *   使用更复杂的测试时数据增强 (Test-Time Augmentation, TTA) 技术。
    *   将多个表现优异的模型进行集成 (Ensemble)，以获得更稳定的结果。 